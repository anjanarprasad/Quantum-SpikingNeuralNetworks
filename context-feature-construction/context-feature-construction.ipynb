import os, json, numpy as np, pandas as pd

PROJECT = "/content/qsnn_methane_exec"   # same in every notebook
os.makedirs(PROJECT, exist_ok=True)

for folder in ["00_inputs", "01_outputs", "reports"]:
    os.makedirs(os.path.join(PROJECT, folder), exist_ok=True)

def save_json(path, obj):
    with open(path, "w") as f:
        json.dump(obj, f, indent=2)

def arr_report(name, X):
    X = np.asarray(X)
    finite = np.isfinite(X)
    out = {
        "name": name,
        "shape": list(X.shape),
        "dtype": str(X.dtype),
        "finite_frac": float(finite.mean()),
        "n_nan": int(np.isnan(X).sum()),
        "n_inf": int(np.isinf(X).sum()),
    }
    xf = X[finite]
    if xf.size:
        out.update({
            "min": float(xf.min()),
            "p1": float(np.percentile(xf, 1)),
            "p50": float(np.percentile(xf, 50)),
            "p99": float(np.percentile(xf, 99)),
            "max": float(xf.max()),
        })
    print(json.dumps(out, indent=2))
    return out
  import os, json, numpy as np, pandas as pd
from sklearn.preprocessing import StandardScaler

PROJECT = "/content/qsnn_methane_exec"

def save_json(path, obj):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w") as f:
        json.dump(obj, f, indent=2)

def assert_finite(name, X):
    X = np.asarray(X)
    if not np.isfinite(X).all():
        raise ValueError(f"{name} non-finite: nan={np.isnan(X).sum()} inf={np.isinf(X).sum()} shape={X.shape}")

# Load 02 output (Colab convention), fallback to /mnt/data if needed
in_path = os.path.join(PROJECT, "01_outputs", "02_zero_inflation_augmented.parquet")
if not os.path.exists(in_path):
    in_path = "/mnt/data/02_zero_inflation_augmented.parquet"

df = pd.read_parquet(in_path)
print("Loaded:", in_path, "| shape:", df.shape)

time_col = "Time" if "Time" in df.columns else ("time" if "time" in df.columns else None)
if time_col is None:
    raise KeyError("No Time column found.")

target_col = "tracer_concentration"
for c in ["zero_mask", "zero_confidence"]:
    if c not in df.columns:
        raise KeyError(f"Missing required column: {c}")

assert df[time_col].is_monotonic_increasing
print("Time monotonic OK.")
cut = int(0.80 * len(df))
train_df = df.iloc[:cut].copy()
test_df  = df.iloc[cut:].copy()
print("Train:", train_df.shape, "| Test:", test_df.shape)

BASE_ENV_COLS = [
    "u_west_to_east_wind",
    "v_south_to_north_wind",
    "temprature",
    "relative_humidity",
    "vertical_velocity",
    "pressure",
    "water_vapour",
    "turbulent_kinatic_energy",
    "precipitation_rate",
    "sensible_heat_flux",
    "Latent_heat_flux",
    "latitude",
    "longitude",
    "i_value",
    "j_value",
]
BASE_ENV_COLS = [c for c in BASE_ENV_COLS if c in df.columns]
print("Using env cols:", BASE_ENV_COLS)
# Wind derived
u = df["u_west_to_east_wind"].astype(float).values if "u_west_to_east_wind" in df.columns else np.zeros(len(df))
v = df["v_south_to_north_wind"].astype(float).values if "v_south_to_north_wind" in df.columns else np.zeros(len(df))
wind_speed = np.sqrt(u*u + v*v)
wind_dir = np.arctan2(v, u)  # radians
df["wind_speed"] = wind_speed.astype(np.float32)
df["wind_dir_sin"] = np.sin(wind_dir).astype(np.float32)
df["wind_dir_cos"] = np.cos(wind_dir).astype(np.float32)

# Stabilize heavy-tailed vars (safe for zeros)
if "turbulent_kinatic_energy" in df.columns:
    df["tke_log1p"] = np.log1p(np.maximum(df["turbulent_kinatic_energy"].astype(float).values, 0.0)).astype(np.float32)
if "precipitation_rate" in df.columns:
    df["precip_log1p"] = np.log1p(np.maximum(df["precipitation_rate"].astype(float).values, 0.0)).astype(np.float32)

# Heat flux combined
if "sensible_heat_flux" in df.columns and "Latent_heat_flux" in df.columns:
    df["total_heat_flux"] = (df["sensible_heat_flux"].astype(float).values + df["Latent_heat_flux"].astype(float).values).astype(np.float32)

# Time cyclic features
t = pd.to_datetime(df[time_col], errors="coerce")
hour = t.dt.hour.values.astype(np.float32)
doy  = t.dt.dayofyear.values.astype(np.float32)
df["hour_sin"] = np.sin(2*np.pi*hour/24.0).astype(np.float32)
df["hour_cos"] = np.cos(2*np.pi*hour/24.0).astype(np.float32)
df["doy_sin"]  = np.sin(2*np.pi*doy/365.25).astype(np.float32)
df["doy_cos"]  = np.cos(2*np.pi*doy/365.25).astype(np.float32)

# Update derived list
DERIVED_COLS = [c for c in ["wind_speed","wind_dir_sin","wind_dir_cos","tke_log1p","precip_log1p","total_heat_flux",
                           "hour_sin","hour_cos","doy_sin","doy_cos"] if c in df.columns]
print("Derived cols:", DERIVED_COLS)
# Re-create train/test AFTER derived features are added to df
cut = int(0.80 * len(df))
train_df = df.iloc[:cut].copy()
test_df  = df.iloc[cut:].copy()
print("Re-split after derived features. Train:", train_df.shape, "| Test:", test_df.shape)
BASE_ENV_COLS = [
    "u_west_to_east_wind","v_south_to_north_wind","temprature","relative_humidity",
    "vertical_velocity","pressure","water_vapour","turbulent_kinatic_energy",
    "precipitation_rate","sensible_heat_flux","Latent_heat_flux",
    "latitude","longitude","i_value","j_value",
]
BASE_ENV_COLS = [c for c in BASE_ENV_COLS if c in df.columns]

DERIVED_CANDIDATES = [
    "wind_speed","wind_dir_sin","wind_dir_cos",
    "tke_log1p","precip_log1p","total_heat_flux",
    "hour_sin","hour_cos","doy_sin","doy_cos",
]
DERIVED_COLS = [c for c in DERIVED_CANDIDATES if c in df.columns]

CTX_COLS = [c for c in (BASE_ENV_COLS + DERIVED_COLS) if c != "tracer_concentration"]
CTX_COLS = list(dict.fromkeys(CTX_COLS))

print("CTX_COLS count:", len(CTX_COLS))
print("Missing in train:", [c for c in CTX_COLS if c not in train_df.columns])
from sklearn.preprocessing import StandardScaler
import numpy as np

scaler = StandardScaler()

Xtr = train_df[CTX_COLS].astype(float).values
Xte = test_df[CTX_COLS].astype(float).values

scaler.fit(Xtr)  # train-only
Xtr_ctx = scaler.transform(Xtr).astype(np.float32)
Xte_ctx = scaler.transform(Xte).astype(np.float32)

print("Context matrices:", Xtr_ctx.shape, Xte_ctx.shape)
# Context feature set = base env + derived (exclude target)
CTX_COLS = [c for c in (BASE_ENV_COLS + DERIVED_COLS) if c != target_col]
CTX_COLS = list(dict.fromkeys(CTX_COLS))  # unique, keep order

# Fit scaler on TRAIN ONLY
scaler = StandardScaler()
Xtr = train_df[CTX_COLS].astype(float).values
Xte = test_df[CTX_COLS].astype(float).values
scaler.fit(Xtr)

Xtr_ctx = scaler.transform(Xtr).astype(np.float32)
Xte_ctx = scaler.transform(Xte).astype(np.float32)

assert_finite("Xtr_ctx", Xtr_ctx)
assert_finite("Xte_ctx", Xte_ctx)

# Confidence propagation v1 (lightweight, explainable):
# start from zero_confidence and reduce confidence if there are time gaps or extreme context outliers.
base_conf = df["zero_confidence"].astype(np.float32).values

# time gap flag (train-based median dt)
t_ns = pd.to_datetime(df[time_col]).values.astype("datetime64[ns]")
dt = np.diff(t_ns).astype("timedelta64[ns]").astype(np.int64) / 1e9
dt = np.concatenate([[0.0], dt]).astype(np.float32)
dt_med = np.median(dt[1:cut][dt[1:cut] > 0]) if np.any(dt[1:cut] > 0) else 0.0
gap_flag = (dt > (5.0 * dt_med)).astype(np.float32) if dt_med > 0 else np.zeros_like(dt, dtype=np.float32)

# outlier flag: any |z|>4 on scaled context (approx; compute in chunks for safety)
Xall = scaler.transform(df[CTX_COLS].astype(float).values).astype(np.float32)
outlier_flag = (np.max(np.abs(Xall), axis=1) > 4.0).astype(np.float32)

ctx_conf = np.clip(base_conf - 0.3*gap_flag - 0.2*outlier_flag, 0.0, 1.0).astype(np.float32)

print("ctx_conf p10/p50/p90:", np.percentile(ctx_conf, [10,50,90]))
zm = df["zero_mask"].astype(np.int8).values

# length of current zero-run ending at i
run = np.zeros(len(zm), dtype=np.int32)
r = 0
for i in range(len(zm)):
    if zm[i] == 1:
        r += 1
    else:
        r = 0
    run[i] = r

# normalize run length using TRAIN stats only (no leakage)
cut = int(0.80 * len(df))
run_tr = run[:cut].astype(np.float32)
p50, p95 = np.percentile(run_tr, 50), np.percentile(run_tr, 95)
den = max(1.0, (p95 - p50))
run_n = np.clip((run.astype(np.float32) - p50) / den, 0.0, 1.0)

# confidence: penalize long zero-runs (only when zero_mask=1)
base_conf = df["zero_confidence"].astype(np.float32).values
ctx_conf = np.clip(base_conf - 0.5 * run_n * (zm == 1), 0.0, 1.0).astype(np.float32)

df["ctx_confidence"] = ctx_conf
print("ctx_conf p10/p50/p90:", np.percentile(ctx_conf, [10,50,90]))
print("ctx_conf (zeros) p10/p50/p90:", np.percentile(ctx_conf[zm==1], [10,50,90]))
conf = df["ctx_confidence"].values
print("Fraction conf < 0.9:", float((conf < 0.9).mean()))
print("Fraction conf < 0.8:", float((conf < 0.8).mean()))
# =========================
# 03 Context: SAVE OUTPUTS
# =========================
import os, json, numpy as np, pandas as pd

PROJECT = "/content/qsnn_methane_exec"
os.makedirs(os.path.join(PROJECT, "01_outputs"), exist_ok=True)
os.makedirs(os.path.join(PROJECT, "reports"), exist_ok=True)

def save_json(path, obj):
    with open(path, "w") as f:
        json.dump(obj, f, indent=2)

# --- Safety checks (optional but recommended) ---
assert "ctx_confidence" in df.columns, "ctx_confidence missing. Run the confidence block first."
assert "zero_mask" in df.columns, "zero_mask missing."
assert "zero_confidence" in df.columns, "zero_confidence missing."
assert np.isfinite(df["ctx_confidence"].values).all(), "ctx_confidence has non-finite values."

# If you have these variables in your notebook, they will be saved:
# - Xtr_ctx, Xte_ctx (scaled context matrices)
# - ctx_conf (vector)
# - CTX_COLS, DERIVED_COLS (lists)
# - in_path, dt_med, cut

# 1) Save augmented dataframe (includes derived cols + ctx_confidence)
out_parquet = os.path.join(PROJECT, "01_outputs", "03_context_augmented.parquet")
df.to_parquet(out_parquet, index=False)
print("Wrote:", out_parquet)

# 2) Save train/test context feature matrices (scaled, train-fitted)
# If you computed Xtr_ctx/Xte_ctx, save them
if "Xtr_ctx" in globals() and "Xte_ctx" in globals():
    np.save(os.path.join(PROJECT, "01_outputs", "03_context_features_train.npy"), Xtr_ctx.astype(np.float32))
    np.save(os.path.join(PROJECT, "01_outputs", "03_context_features_test.npy"),  Xte_ctx.astype(np.float32))
    print("Wrote: 03_context_features_train.npy, 03_context_features_test.npy")
else:
    print("NOTE: Xtr_ctx/Xte_ctx not found in globals(). Not saving context matrices.")

# 3) Save context confidence vector
if "ctx_conf" in globals():
    np.save(os.path.join(PROJECT, "01_outputs", "03_context_confidence.npy"), ctx_conf.astype(np.float32))
    print("Wrote: 03_context_confidence.npy")
else:
    # fallback: save from df column
    np.save(os.path.join(PROJECT, "01_outputs", "03_context_confidence.npy"),
            df["ctx_confidence"].astype(np.float32).values)
    print("Wrote: 03_context_confidence.npy (from df['ctx_confidence'])")

# 4) Save a compact report for traceability
time_col = "Time" if "Time" in df.columns else ("time" if "time" in df.columns else None)

report = {
    "stage": "03_context_feature_construction",
    "input_path": globals().get("in_path", None),
    "output_parquet": out_parquet,
    "n_rows": int(len(df)),
    "n_cols": int(df.shape[1]),
    "time_col": time_col,
    "ctx_cols": globals().get("CTX_COLS", None),
    "derived_cols": globals().get("DERIVED_COLS", None),
    "train_rows": int(globals().get("cut", int(0.80 * len(df)))),
    "ctx_conf_p10_p50_p90": [float(x) for x in np.percentile(df["ctx_confidence"].values, [10, 50, 90])],
    "ctx_conf_lt_0_9": float((df["ctx_confidence"].values < 0.9).mean()),
    "ctx_conf_lt_0_8": float((df["ctx_confidence"].values < 0.8).mean()),
    "dt_median_seconds_train": float(globals().get("dt_med", 0.0)),
}

report_path = os.path.join(PROJECT, "reports", "03_context_report.json")
save_json(report_path, report)
print("Wrote:", report_path)

# 5) Print file sizes (quick confirmation)
for fn in [
    "03_context_augmented.parquet",
    "03_context_features_train.npy",
    "03_context_features_test.npy",
    "03_context_confidence.npy",
]:
    fp = os.path.join(PROJECT, "01_outputs", fn)
    if os.path.exists(fp):
        print(f"OK  {fn}  |  {os.path.getsize(fp)/1e6:.2f} MB")

